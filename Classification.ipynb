{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d64bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/salemi/Documents/Kaggle/happywhale\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "import mlflow\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Custom library in dev\n",
    "import happy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42084793",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
       " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_logical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ccf9850",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/salemi/Documents/Kaggle/happywhale/mlruns\n",
      "/home/salemi/Documents/Kaggle/happywhale/mlruns\n",
      "/home/salemi/Documents/Kaggle/happywhale/mlruns\n",
      "TROLL\n",
      "TROLL\n",
      "TROLL\n"
     ]
    }
   ],
   "source": [
    "print(hp.utils.Config.MLFLOW_URI)\n",
    "print(hp.config.Config.MLFLOW_URI)\n",
    "print(hp.Config.MLFLOW_URI)\n",
    "\n",
    "hp.Config.MLFLOW_URI = \"TROLL\"\n",
    "\n",
    "print(hp.utils.Config.MLFLOW_URI)\n",
    "print(hp.config.Config.MLFLOW_URI)\n",
    "print(hp.Config.MLFLOW_URI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac7b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter dictionnary\n",
    "P = {}\n",
    "\n",
    "P[\"TEST_RUN\"] = True\n",
    "\n",
    "P[\"TRAIN_CSV\"] = \"input/happy-whale-and-dolphin/train.csv\"\n",
    "\n",
    "P[\"TRAIN_FOLDER\"] = \"input/happy-whale-and-dolphin/train_images\"\n",
    "\n",
    "P[\"BATCH_SIZE\"] = 32\n",
    "\n",
    "P[\"EPOCHS\"] = 10\n",
    "\n",
    "P[\"LEARNING_RATE\"] = 1e-3\n",
    "\n",
    "P[\"LEARNING_RATE_FINETUNING\"] = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe89272",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(P[\"TRAIN_CSV\"])\n",
    "\n",
    "if P[\"TEST_RUN\"]:\n",
    "    data_df = data_df.iloc[:1500]\n",
    "\n",
    "species, counts = np.unique(data_df[\"species\"], return_counts=True)\n",
    "\n",
    "P[\"CUTOFF\"] = int(np.floor(np.max(counts) * 0.07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.print_class_statistics(data_df, \"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af20653e",
   "metadata": {},
   "source": [
    "The classes in this dataset, the column \"species\", are too imbalanced. Let's group some less represented classes to get something significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes are too much imbalanced, let's group the one with few example\n",
    "unique_species, count_species = np.unique(data_df[\"species\"], return_counts=True)\n",
    "\n",
    "unique_species = unique_species[np.argsort(count_species)]\n",
    "count_species = count_species[np.argsort(count_species)]\n",
    "\n",
    "map_species = {}\n",
    "idx = 0 \n",
    "acc = 0\n",
    "name = []\n",
    "for i, j in zip(count_species, unique_species):\n",
    "    acc += i\n",
    "    name.append(j)    \n",
    "    if acc >= P[\"CUTOFF\"]:\n",
    "        for n in name:\n",
    "            map_species[n] = idx\n",
    "        idx += 1\n",
    "        acc = 0\n",
    "        name = []\n",
    "            \n",
    "        \n",
    "    \n",
    "data_df[\"class\"] = data_df.apply(lambda x: map_species[x[\"species\"]], axis=1)\n",
    "        \n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(\"preprocessed_224-224/data.h5\") as f:\n",
    "    np_data = f[\"img\"][:]\n",
    "    \n",
    "with tf.device(\"/device:CPU:0\"):\n",
    "    np_data = tf.convert_to_tensor(np_data, dtype=tf.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c12196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083e75ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3a514",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.Dataset.from_tensor_slices(np_data)\n",
    "\n",
    "for i in ds.take(2):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8508e2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c36ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e48ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "hp.print_class_statistics(data_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093426a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 80 / 20 split\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "train_index, val_index = next(skf.split(np.zeros(len(data_df)), data_df[\"class\"].values))\n",
    "\n",
    "train_df = data_df.loc[train_index].copy()\n",
    "val_df = data_df.loc[val_index].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Stats for train set:\")\n",
    "hp.print_class_statistics(train_df, \"class\")\n",
    "\n",
    "print(\"\\nStats for val set:\")\n",
    "hp.print_class_statistics(val_df, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d1ae28",
   "metadata": {},
   "source": [
    "The splits looks fairly good with a conserved class prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShardedGenerator:\n",
    "    def __init__(self, df: pd.DataFrame, n_shards: int = None):\n",
    "        \n",
    "        self._df = df.copy()\n",
    "                \n",
    "        self._n_shards = n_shards\n",
    "        if self._n_shards is None:\n",
    "            self._n_shards = cpu_count()\n",
    "            \n",
    "        self._df[\"filepath\"] = self._df.apply(lambda x: os.path.join(P[\"TRAIN_FOLDER\"], x[\"image\"]), axis=1)\n",
    "        \n",
    "        \n",
    "    def __call__(self, n):\n",
    "        with h5.File(\"preprocessed_224-224/data.h5\", \"r\") as f:\n",
    "            for count, (i, row) in enumerate(self._df.iterrows()):\n",
    "                if count % self._n_shards != n:\n",
    "                    continue\n",
    "                    \n",
    "                img = tf.convert_to_tensor(f[\"img\"][i])\n",
    "                \n",
    "                label = tf.convert_to_tensor(row[\"class\"], dtype=tf.int64)\n",
    "\n",
    "                yield img, label\n",
    "                \n",
    "                \n",
    "gen = ShardedGenerator(train_df, 1)\n",
    "\n",
    "for i, j in gen(0):\n",
    "    print(i, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d6f83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data:pd.DataFrame, n_shards=4):\n",
    "    gen = ShardedGenerator(data, n_shards)\n",
    "\n",
    "    out_sign = (tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(np.arange(n_shards))\n",
    "\n",
    "    ds = ds.interleave(lambda x: tf.data.Dataset.from_generator(gen, output_signature=out_sign, args=(x,)),\n",
    "                       cycle_length=n_shards,\n",
    "                       block_length=1,\n",
    "                       num_parallel_calls=n_shards,\n",
    "                       deterministic=True)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "P[\"STEPS_PER_EPOCH\"] = int(np.ceil(len(train_df) / P[\"BATCH_SIZE\"]))\n",
    "\n",
    "\n",
    "ds_train = get_dataset(train_df, n_shards=16)\n",
    "ds_train = ds_train.batch(P[\"BATCH_SIZE\"]).cache()\n",
    "ds_train = ds_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE).repeat(P[\"EPOCHS\"])\n",
    "\n",
    "\n",
    "ds_val = get_dataset(val_df, n_shards=16)\n",
    "ds_val = ds_val.batch(P[\"BATCH_SIZE\"]).cache()\n",
    "ds_val = ds_val.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65756c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(train_df[\"class\"].unique())\n",
    "print(num_classes)\n",
    "\n",
    "class_weight = compute_class_weight(\"balanced\", classes=np.arange(num_classes), y=train_df[\"class\"])\n",
    "class_weight = dict({i:class_weight[i] for i in range(len(class_weight))})\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be166a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(224, 224, 3)),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\",trainable=False),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777272f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36eadaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08483a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x = np.random.rand(1, 224, 224, 3).astype(np.float32)\n",
    "y = model.predict(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04af9aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50c4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f0bd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce8765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"prepend\"\n",
    "y = x if x else \"DEFAULT\"\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b272df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflowCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, prepend: str = None):\n",
    "        self._prepend = prepend if prepend else \"\"\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            for k, v in logs.items():\n",
    "                mlflow.log_metric(key=self._prepend + k, value=v, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9fd79f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = \"classification\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if experiment is not None:\n",
    "    mlflow.delete_experiment(experiment.experiment_id)\n",
    "    hp.config.clean_mlflow_trash()\n",
    "\n",
    "exp_id = mlflow.create_experiment(name=exp_name)    \n",
    "        \n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id) as run:\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=P[\"LEARNING_RATE\"]),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(ds_train,\n",
    "                        validation_data=ds_val,\n",
    "                        epochs=P[\"EPOCHS\"], \n",
    "                        steps_per_epoch=P[\"STEPS_PER_EPOCH\"],\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[MLflowCallback()])\n",
    "    \n",
    "    mlflow.log_dict(history.history, \"history.json\")\n",
    "    \n",
    "    # Allow finetuning\n",
    "    for l in model.layers:\n",
    "        l.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=P[\"LEARNING_RATE_FINETUNING\"]),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(ds_train,\n",
    "                        validation_data=ds_val,\n",
    "                        epochs=P[\"EPOCHS\"], \n",
    "                        steps_per_epoch=P[\"STEPS_PER_EPOCH\"],\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[MLflowCallback(prepend=\"finetune-\")])\n",
    "                                                                     \n",
    "    mlflow.log_dict(history.history, \"history-finetune.json\")\n",
    "    \n",
    "    mlflow.log_dict(P, \"params.yaml\")\n",
    "                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f892ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runs = mlflow.search_runs(exp_id, output_format=\"list\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c3268e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.tracking.MlflowClient()\n",
    "\n",
    "metrics = client.get_metric_history(runs[0].info.run_id, \"accuracy\")\n",
    "df = pd.DataFrame()\n",
    "for i in metrics:\n",
    "    df = df.append(dict(i), ignore_index=True)\n",
    "df.head()\n",
    "\n",
    "plt.plot(df.step, df.value, \"o-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c61b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "i, j = model.layers[-1].get_weights()\n",
    "\n",
    "print(i.shape, j.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
