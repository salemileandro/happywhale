{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16f2b87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/salemi/Documents/Kaggle/happywhale\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "import mlflow\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Custom library in dev\n",
    "import happy as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a102d3b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d27ea42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter dictionnary\n",
    "P = {}\n",
    "\n",
    "P[\"TEST_RUN\"] = True\n",
    "\n",
    "P[\"TRAIN_CSV\"] = \"input/happy-whale-and-dolphin/train.csv\"\n",
    "\n",
    "P[\"TRAIN_FOLDER\"] = \"input/happy-whale-and-dolphin/train_images\"\n",
    "\n",
    "P[\"BATCH_SIZE\"] = 32\n",
    "\n",
    "P[\"EPOCHS\"] = 10\n",
    "\n",
    "P[\"LEARNING_RATE\"] = 1e-3\n",
    "\n",
    "P[\"LEARNING_RATE_FINETUNING\"] = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f8d50ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(P[\"TRAIN_CSV\"])\n",
    "\n",
    "if P[\"TEST_RUN\"]:\n",
    "    data_df = data_df.iloc[:1500]\n",
    "\n",
    "species, counts = np.unique(data_df[\"species\"], return_counts=True)\n",
    "\n",
    "P[\"CUTOFF\"] = int(np.floor(np.max(counts) * 0.07))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "038d9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS NAME                    COUNTS    PERCENTAGE\n",
      "--------------------------------------------------\n",
      "bottlenose_dolphin            284       18.93     \n",
      "beluga                        227       15.13     \n",
      "humpback_whale                207       13.80     \n",
      "blue_whale                    135       9.00      \n",
      "false_killer_whale            103       6.87      \n",
      "dusky_dolphin                 86        5.73      \n",
      "minke_whale                   51        3.40      \n",
      "melon_headed_whale            50        3.33      \n",
      "spinner_dolphin               47        3.13      \n",
      "killer_whale                  44        2.93      \n",
      "gray_whale                    34        2.27      \n",
      "fin_whale                     33        2.20      \n",
      "southern_right_whale          30        2.00      \n",
      "bottlenose_dolpin             29        1.93      \n",
      "kiler_whale                   28        1.87      \n",
      "spotted_dolphin               21        1.40      \n",
      "short_finned_pilot_whale      16        1.07      \n",
      "sei_whale                     13        0.87      \n",
      "cuviers_beaked_whale          11        0.73      \n",
      "common_dolphin                11        0.73      \n",
      "pilot_whale                   9         0.60      \n",
      "long_finned_pilot_whale       6         0.40      \n",
      "brydes_whale                  6         0.40      \n",
      "commersons_dolphin            5         0.33      \n",
      "white_sided_dolphin           5         0.33      \n",
      "globis                        3         0.20      \n",
      "pantropic_spotted_dolphin     3         0.20      \n",
      "rough_toothed_dolphin         3         0.20      \n"
     ]
    }
   ],
   "source": [
    "hp.print_class_statistics(data_df, \"species\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9896cf5",
   "metadata": {},
   "source": [
    "The classes in this dataset, the column \"species\", are too imbalanced. Let's group some less represented classes to get something significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c482533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>species</th>\n",
       "      <th>individual_id</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00021adfb725ed.jpg</td>\n",
       "      <td>melon_headed_whale</td>\n",
       "      <td>cadddb1636b9</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000562241d384d.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>1a71fbb72250</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0007c33415ce37.jpg</td>\n",
       "      <td>false_killer_whale</td>\n",
       "      <td>60008f293a2b</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0007d9bca26a99.jpg</td>\n",
       "      <td>bottlenose_dolphin</td>\n",
       "      <td>4b00fe572063</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00087baf5cef7a.jpg</td>\n",
       "      <td>humpback_whale</td>\n",
       "      <td>8e5253662392</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                image             species individual_id  class\n",
       "0  00021adfb725ed.jpg  melon_headed_whale  cadddb1636b9     12\n",
       "1  000562241d384d.jpg      humpback_whale  1a71fbb72250     17\n",
       "2  0007c33415ce37.jpg  false_killer_whale  60008f293a2b     15\n",
       "3  0007d9bca26a99.jpg  bottlenose_dolphin  4b00fe572063     19\n",
       "4  00087baf5cef7a.jpg      humpback_whale  8e5253662392     17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Classes are too much imbalanced, let's group the one with few example\n",
    "unique_species, count_species = np.unique(data_df[\"species\"], return_counts=True)\n",
    "\n",
    "unique_species = unique_species[np.argsort(count_species)]\n",
    "count_species = count_species[np.argsort(count_species)]\n",
    "\n",
    "map_species = {}\n",
    "idx = 0 \n",
    "acc = 0\n",
    "name = []\n",
    "for i, j in zip(count_species, unique_species):\n",
    "    acc += i\n",
    "    name.append(j)    \n",
    "    if acc >= P[\"CUTOFF\"]:\n",
    "        for n in name:\n",
    "            map_species[n] = idx\n",
    "        idx += 1\n",
    "        acc = 0\n",
    "        name = []\n",
    "            \n",
    "        \n",
    "    \n",
    "data_df[\"class\"] = data_df.apply(lambda x: map_species[x[\"species\"]], axis=1)\n",
    "        \n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53234773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASS NAME                    COUNTS    PERCENTAGE\n",
      "--------------------------------------------------\n",
      "19                            284       18.93     \n",
      "18                            227       15.13     \n",
      "17                            207       13.80     \n",
      "16                            135       9.00      \n",
      "15                            103       6.87      \n",
      "14                            86        5.73      \n",
      "13                            51        3.40      \n",
      "12                            50        3.33      \n",
      "11                            47        3.13      \n",
      "10                            44        2.93      \n",
      "9                             34        2.27      \n",
      "8                             33        2.20      \n",
      "7                             30        2.00      \n",
      "6                             29        1.93      \n",
      "3                             29        1.93      \n",
      "5                             28        1.87      \n",
      "2                             22        1.47      \n",
      "4                             21        1.40      \n",
      "1                             21        1.40      \n",
      "0                             19        1.27      \n"
     ]
    }
   ],
   "source": [
    "hp.print_class_statistics(data_df, \"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a15756",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats for train set:\n",
      "CLASS NAME                    COUNTS    PERCENTAGE\n",
      "--------------------------------------------------\n",
      "19                            227       18.92     \n",
      "18                            181       15.08     \n",
      "17                            165       13.75     \n",
      "16                            108       9.00      \n",
      "15                            83        6.92      \n",
      "14                            68        5.67      \n",
      "13                            41        3.42      \n",
      "12                            40        3.33      \n",
      "11                            37        3.08      \n",
      "10                            36        3.00      \n",
      "8                             27        2.25      \n",
      "9                             27        2.25      \n",
      "7                             24        2.00      \n",
      "6                             24        2.00      \n",
      "5                             23        1.92      \n",
      "3                             23        1.92      \n",
      "2                             17        1.42      \n",
      "1                             17        1.42      \n",
      "4                             16        1.33      \n",
      "0                             16        1.33      \n",
      "\n",
      "Stats for val set:\n",
      "CLASS NAME                    COUNTS    PERCENTAGE\n",
      "--------------------------------------------------\n",
      "19                            57        19.00     \n",
      "18                            46        15.33     \n",
      "17                            42        14.00     \n",
      "16                            27        9.00      \n",
      "15                            20        6.67      \n",
      "14                            18        6.00      \n",
      "13                            10        3.33      \n",
      "12                            10        3.33      \n",
      "11                            10        3.33      \n",
      "10                            8         2.67      \n",
      "9                             7         2.33      \n",
      "8                             6         2.00      \n",
      "7                             6         2.00      \n",
      "3                             6         2.00      \n",
      "6                             5         1.67      \n",
      "5                             5         1.67      \n",
      "4                             5         1.67      \n",
      "2                             5         1.67      \n",
      "1                             4         1.33      \n",
      "0                             3         1.00      \n"
     ]
    }
   ],
   "source": [
    "# 80 / 20 split\n",
    "skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "train_index, val_index = next(skf.split(np.zeros(len(data_df)), data_df[\"class\"].values))\n",
    "\n",
    "train_df = data_df.loc[train_index].copy()\n",
    "val_df = data_df.loc[val_index].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Stats for train set:\")\n",
    "hp.print_class_statistics(train_df, \"class\")\n",
    "\n",
    "print(\"\\nStats for val set:\")\n",
    "hp.print_class_statistics(val_df, \"class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e0038b",
   "metadata": {},
   "source": [
    "The splits looks fairly good with a conserved class prevalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c6e6ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 76 122 171]\n",
      "  [ 76 124 170]\n",
      "  [ 84 127 174]\n",
      "  ...\n",
      "  [ 62 100 147]\n",
      "  [ 63  99 151]\n",
      "  [ 60 101 150]]\n",
      "\n",
      " [[ 89 136 178]\n",
      "  [ 86 134 174]\n",
      "  [ 94 139 179]\n",
      "  ...\n",
      "  [ 49  86 131]\n",
      "  [ 53  87 136]\n",
      "  [ 56  90 138]]\n",
      "\n",
      " [[ 87 132 174]\n",
      "  [ 92 137 178]\n",
      "  [ 90 136 177]\n",
      "  ...\n",
      "  [ 58  92 137]\n",
      "  [ 59  93 138]\n",
      "  [ 60  92 139]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 47  44  53]\n",
      "  [ 47  46  54]\n",
      "  [ 44  44  52]\n",
      "  ...\n",
      "  [ 48  47  55]\n",
      "  [ 47  47  55]\n",
      "  [ 49  46  55]]\n",
      "\n",
      " [[ 43  43  51]\n",
      "  [ 42  42  50]\n",
      "  [ 47  46  54]\n",
      "  ...\n",
      "  [ 49  48  56]\n",
      "  [ 48  48  56]\n",
      "  [ 48  47  55]]\n",
      "\n",
      " [[ 40  43  52]\n",
      "  [ 43  43  53]\n",
      "  [ 43  41  52]\n",
      "  ...\n",
      "  [ 51  50  59]\n",
      "  [ 52  51  59]\n",
      "  [ 49  49  57]]], shape=(224, 224, 3), dtype=uint8) tf.Tensor(12, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "class ShardedGenerator:\n",
    "    def __init__(self, df: pd.DataFrame, n_shards: int = None):\n",
    "        \n",
    "        self._df = df.copy()\n",
    "                \n",
    "        self._n_shards = n_shards\n",
    "        if self._n_shards is None:\n",
    "            self._n_shards = cpu_count()\n",
    "            \n",
    "        self._df[\"filepath\"] = self._df.apply(lambda x: os.path.join(P[\"TRAIN_FOLDER\"], x[\"image\"]), axis=1)\n",
    "        \n",
    "        \n",
    "    def __call__(self, n):\n",
    "        with h5.File(\"preprocessed_224-224/data.h5\", \"r\") as f:\n",
    "            for count, (i, row) in enumerate(self._df.iterrows()):\n",
    "                if count % self._n_shards != n:\n",
    "                    continue\n",
    "                    \n",
    "                img = tf.convert_to_tensor(f[\"img\"][i])\n",
    "                \n",
    "                label = tf.convert_to_tensor(row[\"class\"], dtype=tf.int64)\n",
    "\n",
    "                yield img, label\n",
    "                \n",
    "                \n",
    "gen = ShardedGenerator(train_df, 1)\n",
    "\n",
    "for i, j in gen(0):\n",
    "    print(i, j)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49997a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(data:pd.DataFrame, n_shards=4):\n",
    "    gen = ShardedGenerator(data, n_shards)\n",
    "\n",
    "    out_sign = (tf.TensorSpec(shape=(224, 224, 3), dtype=tf.uint8), tf.TensorSpec(shape=(), dtype=tf.int64))\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices(np.arange(n_shards))\n",
    "\n",
    "    ds = ds.interleave(lambda x: tf.data.Dataset.from_generator(gen, output_signature=out_sign, args=(x,)),\n",
    "                       cycle_length=n_shards,\n",
    "                       block_length=1,\n",
    "                       num_parallel_calls=n_shards,\n",
    "                       deterministic=True)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "\n",
    "P[\"STEPS_PER_EPOCH\"] = int(np.ceil(len(train_df) / P[\"BATCH_SIZE\"]))\n",
    "\n",
    "\n",
    "ds_train = get_dataset(train_df, n_shards=16)\n",
    "ds_train = ds_train.batch(P[\"BATCH_SIZE\"]).cache()\n",
    "ds_train = ds_train.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(tf.data.AUTOTUNE).repeat(P[\"EPOCHS\"])\n",
    "\n",
    "\n",
    "ds_val = get_dataset(val_df, n_shards=16)\n",
    "ds_val = ds_val.batch(P[\"BATCH_SIZE\"]).cache()\n",
    "ds_val = ds_val.map(lambda x, y: (tf.image.convert_image_dtype(x, tf.float32), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "ds_val = ds_val.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfead9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "{0: 3.75, 1: 3.5294117647058822, 2: 3.5294117647058822, 3: 2.608695652173913, 4: 3.75, 5: 2.608695652173913, 6: 2.5, 7: 2.5, 8: 2.2222222222222223, 9: 2.2222222222222223, 10: 1.6666666666666667, 11: 1.6216216216216217, 12: 1.5, 13: 1.4634146341463414, 14: 0.8823529411764706, 15: 0.7228915662650602, 16: 0.5555555555555556, 17: 0.36363636363636365, 18: 0.3314917127071823, 19: 0.2643171806167401}\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(train_df[\"class\"].unique())\n",
    "print(num_classes)\n",
    "\n",
    "class_weight = compute_class_weight(\"balanced\", classes=np.arange(num_classes), y=train_df[\"class\"])\n",
    "class_weight = dict({i:class_weight[i] for i in range(len(class_weight))})\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d43f742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(224, 224, 3)),\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\",trainable=False),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c87e3487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " keras_layer (KerasLayer)    (None, 2048)              23564800  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20)                40980     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,605,780\n",
      "Trainable params: 40,980\n",
      "Non-trainable params: 23,564,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8ed540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d8e56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bbfd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77cec88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e4e79f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e13aaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepend\n"
     ]
    }
   ],
   "source": [
    "x = \"prepend\"\n",
    "y = x if x else \"DEFAULT\"\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4f1a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLflowCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, prepend: str = None):\n",
    "        self._prepend = prepend if prepend else \"\"\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is not None:\n",
    "            for k, v in logs.items():\n",
    "                mlflow.log_metric(key=self._prepend + k, value=v, step=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4764e271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "38/38 [==============================] - 12s 164ms/step - loss: 2.4734 - accuracy: 0.3483 - val_loss: 1.4506 - val_accuracy: 0.5600\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 1.3898 - accuracy: 0.6300 - val_loss: 1.2070 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 1.0466 - accuracy: 0.7317 - val_loss: 1.0825 - val_accuracy: 0.6800\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 2s 65ms/step - loss: 0.8318 - accuracy: 0.7975 - val_loss: 1.0069 - val_accuracy: 0.7233\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 0.6835 - accuracy: 0.8300 - val_loss: 0.9540 - val_accuracy: 0.7367\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 2s 64ms/step - loss: 0.5761 - accuracy: 0.8658 - val_loss: 0.9143 - val_accuracy: 0.7500\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.4934 - accuracy: 0.8892 - val_loss: 0.8831 - val_accuracy: 0.7567\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.4276 - accuracy: 0.9050 - val_loss: 0.8578 - val_accuracy: 0.7600\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 3s 68ms/step - loss: 0.3743 - accuracy: 0.9192 - val_loss: 0.8371 - val_accuracy: 0.7700\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 3s 67ms/step - loss: 0.3305 - accuracy: 0.9400 - val_loss: 0.8205 - val_accuracy: 0.7667\n",
      "Epoch 1/10\n",
      "38/38 [==============================] - 15s 218ms/step - loss: 1.6193 - accuracy: 0.6692 - val_loss: 1.5894 - val_accuracy: 0.6200\n",
      "Epoch 2/10\n",
      "38/38 [==============================] - 7s 178ms/step - loss: 0.4069 - accuracy: 0.9542 - val_loss: 1.1369 - val_accuracy: 0.7400\n",
      "Epoch 3/10\n",
      "38/38 [==============================] - 7s 177ms/step - loss: 0.2858 - accuracy: 0.9942 - val_loss: 1.1061 - val_accuracy: 0.7533\n",
      "Epoch 4/10\n",
      "38/38 [==============================] - 7s 190ms/step - loss: 0.2699 - accuracy: 0.9992 - val_loss: 1.1059 - val_accuracy: 0.7600\n",
      "Epoch 5/10\n",
      "38/38 [==============================] - 7s 193ms/step - loss: 0.2645 - accuracy: 1.0000 - val_loss: 1.1128 - val_accuracy: 0.7633\n",
      "Epoch 6/10\n",
      "38/38 [==============================] - 7s 193ms/step - loss: 0.2617 - accuracy: 1.0000 - val_loss: 1.1142 - val_accuracy: 0.7700\n",
      "Epoch 7/10\n",
      "38/38 [==============================] - 7s 193ms/step - loss: 0.2600 - accuracy: 1.0000 - val_loss: 1.1179 - val_accuracy: 0.7600\n",
      "Epoch 8/10\n",
      "38/38 [==============================] - 7s 193ms/step - loss: 0.2589 - accuracy: 1.0000 - val_loss: 1.1247 - val_accuracy: 0.7633\n",
      "Epoch 9/10\n",
      "38/38 [==============================] - 7s 192ms/step - loss: 0.2580 - accuracy: 1.0000 - val_loss: 1.1314 - val_accuracy: 0.7633\n",
      "Epoch 10/10\n",
      "38/38 [==============================] - 7s 192ms/step - loss: 0.2574 - accuracy: 1.0000 - val_loss: 1.1368 - val_accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"classification\"\n",
    "experiment = mlflow.get_experiment_by_name(exp_name)\n",
    "if experiment is not None:\n",
    "    mlflow.delete_experiment(experiment.experiment_id)\n",
    "    hp.config.clean_mlflow_trash()\n",
    "\n",
    "exp_id = mlflow.create_experiment(name=exp_name)    \n",
    "        \n",
    "\n",
    "with mlflow.start_run(experiment_id=exp_id) as run:\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=P[\"LEARNING_RATE\"]),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(ds_train,\n",
    "                        validation_data=ds_val,\n",
    "                        epochs=P[\"EPOCHS\"], \n",
    "                        steps_per_epoch=P[\"STEPS_PER_EPOCH\"],\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[MLflowCallback()])\n",
    "    \n",
    "    mlflow.log_dict(history.history, \"history.json\")\n",
    "    \n",
    "    # Allow finetuning\n",
    "    for l in model.layers:\n",
    "        l.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=P[\"LEARNING_RATE_FINETUNING\"]),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(ds_train,\n",
    "                        validation_data=ds_val,\n",
    "                        epochs=P[\"EPOCHS\"], \n",
    "                        steps_per_epoch=P[\"STEPS_PER_EPOCH\"],\n",
    "                        class_weight=class_weight,\n",
    "                        callbacks=[MLflowCallback(prepend=\"finetune-\")])\n",
    "                                                                     \n",
    "    mlflow.log_dict(history.history, \"history-finetune.json\")\n",
    "    \n",
    "    mlflow.log_dict(P, \"params.yaml\")\n",
    "                                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6509cce7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>status</th>\n",
       "      <th>artifact_uri</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>metrics.finetune-val_loss</th>\n",
       "      <th>metrics.finetune-val_accuracy</th>\n",
       "      <th>metrics.loss</th>\n",
       "      <th>metrics.accuracy</th>\n",
       "      <th>metrics.finetune-loss</th>\n",
       "      <th>metrics.val_accuracy</th>\n",
       "      <th>metrics.val_loss</th>\n",
       "      <th>metrics.finetune-accuracy</th>\n",
       "      <th>tags.mlflow.source.type</th>\n",
       "      <th>tags.mlflow.source.name</th>\n",
       "      <th>tags.mlflow.user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54eb0cf410994e8586de1613e0b434cf</td>\n",
       "      <td>1</td>\n",
       "      <td>FINISHED</td>\n",
       "      <td>file:///home/salemi/Documents/Kaggle/happywhal...</td>\n",
       "      <td>2022-02-06 00:23:31.132000+00:00</td>\n",
       "      <td>2022-02-06 00:25:26.130000+00:00</td>\n",
       "      <td>1.136807</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.330518</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.257398</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.820492</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LOCAL</td>\n",
       "      <td>/home/salemi/miniconda3/envs/happywhale/lib/py...</td>\n",
       "      <td>salemi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             run_id experiment_id    status  \\\n",
       "0  54eb0cf410994e8586de1613e0b434cf             1  FINISHED   \n",
       "\n",
       "                                        artifact_uri  \\\n",
       "0  file:///home/salemi/Documents/Kaggle/happywhal...   \n",
       "\n",
       "                        start_time                         end_time  \\\n",
       "0 2022-02-06 00:23:31.132000+00:00 2022-02-06 00:25:26.130000+00:00   \n",
       "\n",
       "   metrics.finetune-val_loss  metrics.finetune-val_accuracy  metrics.loss  \\\n",
       "0                   1.136807                       0.766667      0.330518   \n",
       "\n",
       "   metrics.accuracy  metrics.finetune-loss  metrics.val_accuracy  \\\n",
       "0              0.94               0.257398              0.766667   \n",
       "\n",
       "   metrics.val_loss  metrics.finetune-accuracy tags.mlflow.source.type  \\\n",
       "0          0.820492                        1.0                   LOCAL   \n",
       "\n",
       "                             tags.mlflow.source.name tags.mlflow.user  \n",
       "0  /home/salemi/miniconda3/envs/happywhale/lib/py...           salemi  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs = mlflow.search_runs(exp_id)\n",
    "\n",
    "runs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0291301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/salemi/Documents/Kaggle/happywhale/mlflow/1/54eb0cf410994e8586de1613e0b434cf/artifacts'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs.at[0, \"artifact_uri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9ea22d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = int(np.ceil(len(train_df) / P[\"BATCH_SIZE\"]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
